<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-US">
<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<link rel="canonical" href="MindsAnIntroduction.html" />
	<title>Minds: An Introduction </title>
	<meta name="viewport" content="width=device-width, initial-scale=1"/>
	<link rel='stylesheet' href='wiki/pub/skins/readthesequences/skin.css' type='text/css' />
	<!--HTMLHeader--><style type='text/css'><!--a[href^='http://archive.is/timegate/'] { opacity: 0.5;  }

.footnote_block_begin { 
	width: 160px; 
	border-bottom: 1px solid blue;
	margin-bottom: 0.5em;
}
div.footnote {
	margin: 0 3em 0.5em 0;
	padding-left: 2em;
	font-size: 0.9em;
	position: relative;
}
div.footnote .footnote-number {
	position: absolute;
	left: 0;
	width: 1.5em;
	text-align: right;
}
div.footnote .footnote-number::after {
	content: '.';
}
.num { position: relative; font-size: 0.7em; bottom: 0.5em; right: 0.1em; margin-left: 0.15em; }
.frasl { font-size: 1.15em; line-height: 1; }
.denom { position: relative; font-size: 0.7em; top: 0.05em; left: 0.1em; }

--></style><meta http-equiv='Content-Type' content='text/html; charset=utf-8' /><link href="wiki/uploads/favicon.png" type="image/png" rel="shortcut icon" /><link rel='preload' href='wiki/fonts/font_files/GaramondPremierProSubhead/GaramondPremierProSubhead-Medium.otf' type='font/otf' as='font' crossorigin />
<link rel='preload' href='wiki/fonts/font_files/ProximaNova/ProximaNova-Thin.otf' type='font/otf' as='font' crossorigin />
  <meta name='robots' content='index,follow' />

</head>
<body>
<!--PageText-->
<div id='wikitext'>
<div class='article-talk-selector' > 
<p><a target='blank'  class='wikilink' href='MindsAnIntroduction.html' title='View PmWiki source for “Minds: An Introduction”'>Source</a><a target='blank'  class='wikilink' href='MindsAnIntroduction.html' title='View “Minds: An Introduction” in Markdown format'>Markdown</a> · <a rel='nofollow'  class='wikilink' href='Talk/Minds-An-Introduction.html' title='View the Talk page for “Minds: An Introduction”'>Talk</a>
</p></div>
<div class='nav_menu' > 
<p><a class='wikilink' href='HomePage.html'>Home</a><a class='wikilink' href='About.html'>About</a><a class='urllink' href='Search.html' rel='nofollow'>Search</a><a class='wikilink' href='Contents.html'>Contents</a>
</p></div>
<h1 class='h1_with_byline'> Minds: An Introduction</h1>
<p  style='text-align: center;'> by Rob Bensinger
</p>
<p  style='text-align: center;'> ❦
</p>
<p><span class='smallcaps'>You’re a mind</span>, and that puts you in a pretty strange predicament.
</p>
<p>Very few things get to be minds. You’re that odd bit of stuff in the universe that can form predictions and make plans, weigh and revise beliefs, suffer, dream, notice ladybugs, or feel a sudden craving for mango. You can even form, <em>inside your mind</em>, a picture of your whole mind. You can reason about your own reasoning process, and work to bring its operations more in line with your goals.
</p>
<p>You’re a mind, implemented on a human brain. And it turns out that a human brain, for all its marvelous flexibility, is a lawful thing, a thing of pattern and routine. Your mind can follow a routine for a lifetime, without ever once noticing that it is doing so. And these routines can have great consequences.
</p>
<p>When a mental pattern serves you well, we call that “rationality.”
</p>
<p>You exist as you are, hard-wired to exhibit certain species of rationality and certain species of irrationality, because of your ancestry. You, and all life on Earth, are descended from ancient self-replicating molecules. This replication process was initially clumsy and haphazard, and soon yielded replicable <em>differences</em> between the replicators. “Evolution” is our name for the change in these differences over time.
</p>
<p>Since some of these reproducible differences impact reproducibility—a phenomenon called “selection”—evolution has resulted in organisms suited to reproduction in environments like the ones their ancestors had. Everything about you is built on the echoes of your ancestors’ struggles and victories.
</p>
<p>And so here you are: a mind, carved from weaker minds, seeking to understand your own inner workings, that they can be improved upon—improved upon relative to <em>your</em> goals, and not those of your designer, evolution. What useful policies and insights can we take away from knowing that this is our basic situation?
</p>
<p  style='text-align: center;'> ☙
</p>
<h3>Ghosts and Machines</h3>
<p><span class='smallcaps'>Our brains, in their</span> small-scale structure and dynamics, look like many other mechanical systems. Yet we rarely think of our minds in the same terms we think of objects in our environments or organs in our bodies. Our basic mental categories—belief, decision, word, idea, feeling, and so on—bear little resemblance to our physical categories.
</p>
<p>Past philosophers have taken this observation and run with it, arguing that minds and brains are fundamentally distinct and separate phenomena. This is the view the philosopher Gilbert Ryle called “the dogma of the Ghost in the Machine.”<sup><a class='footnote' id='citation1' href='MindsAnIntroduction.html#footnote1'>1</a></sup> But modern scientists and philosophers who have rejected dualism haven’t necessarily replaced it with a better predictive model of how the mind works. <em>Practically</em> speaking, our purposes and desires still function like free-floating ghosts, like a magisterium cut off from the rest of our scientific knowledge. We can talk about “rationality” and “bias” and “how to change our minds,” but if those ideas are still imprecise and unconstrained by any overarching theory, our scientific-sounding language won’t protect us from making the same kinds of mistakes as those whose theoretical posits include spirits and essences.
</p>
<p>Interestingly, the mystery and mystification surrounding minds doesn’t just obscure our view of <em>humans</em>. It also accrues to systems that seem mind-like or purposeful in evolutionary biology and artificial intelligence (AI). Perhaps, if we cannot readily glean what we are from looking at ourselves, we can learn more by using obviously <em>in</em>human processes as a mirror.
</p>
<p>There are many ghosts to learn from here—ghosts past, and present, and yet to come. And these illusions are real cognitive events, real phenomena that we can study and explain. If there <em>appears</em> to be a ghost in the machine, that appearance is itself the hidden work of a machine.
</p>
<p>The first sequence of <em>The Machine in the Ghost</em>, “<a class='wikilink' href='TheSimpleMathOfEvolutionSequence.html'>The Simple Math of Evolution</a>,” aims to communicate the dissonance and divergence between our hereditary history, our present-day biology, and our ultimate aspirations. This will require digging deeper than is common in introductions to evolution for non-biologists, which often restrict their attention to surface-level features of natural selection.
</p>
<p>The third sequence, “<a class='wikilink' href='AHumansGuideToWordsSequence.html'>A Human’s Guide to Words</a>,” discusses the basic relationship between cognition and concept formation. This is followed by <a class='wikilink' href='AnIntuitiveExplanationOfBayessTheorem.html'>a longer essay introducing Bayesian inference</a>.
</p>
<p>Bridging the gap between these topics, “<a class='wikilink' href='FragilePurposesSequence.html'>Fragile Purposes</a>” abstracts from human cognition and evolution to the idea of minds and goal-directed systems at their most general. These essays serve the secondary purpose of explaining the author’s general approach to philosophy and the science of rationality, which is strongly informed by his work in AI.
</p>
<p  style='text-align: center;'> ☙
</p>
<h3>Rebuilding Intelligence</h3>
<p><span class='smallcaps'>Yudkowsky is a</span> decision theorist and mathematician who works on foundational issues in Artificial General Intelligence (AGI), the theoretical study of domain-general problem-solving systems. Yudkowsky’s work in AI has been a major driving force behind his exploration of the psychology of human rationality, as he noted in his very first blog post on <em>Overcoming Bias</em>, <a class='urllink' href='https://www.greaterwrong.com/lw/gn/the_martial_art_of_rationality/' rel='nofollow'>The Martial Art of Rationality</a>:
</p>
<blockquote>
<p>Such understanding as I have of rationality, I acquired in the course of wrestling with the challenge of Artificial General Intelligence (an endeavor which, to actually succeed, would require sufficient mastery of rationality to build a complete working rationalist out of toothpicks and rubber bands). In most ways the AI problem is enormously more demanding than the personal art of rationality, but in some ways it is actually easier. In the martial art of mind, we need to acquire the real-time procedural skill of pulling the right levers at the right time on a large, pre-existing thinking machine whose innards are not end-user-modifiable. Some of the machinery is optimized for evolutionary selection pressures that run directly counter to our declared goals in using it. Deliberately we decide that we want to seek only the truth; but our brains have hardwired support for rationalizing falsehoods. [...]
</p>
<p>Trying to synthesize a personal art of rationality, using the science of rationality, may prove awkward: One imagines trying to invent a martial art using an abstract theory of physics, game theory, and human anatomy. But humans are not reflectively blind; we do have a native instinct for introspection. The inner eye is not sightless; but it sees blurrily, with systematic distortions. We need, then, to apply the science to our intuitions, to use the abstract knowledge to correct our mental movements and augment our metacognitive skills. We are not writing a computer program to make a string puppet execute martial arts forms; it is our own mental limbs that we must move. Therefore we must connect theory to practice. We must come to see what the science means, for ourselves, for our daily inner life.
</p></blockquote>
<p>From Yudkowsky’s perspective, I gather, talking about human rationality without saying anything interesting about AI is about as difficult as talking about AI without saying anything interesting about rationality.
</p>
<p>In the long run, Yudkowsky predicts that AI will come to surpass humans in an “intelligence explosion,” a scenario in which self-modifying AI improves its own ability to productively redesign itself, kicking off a rapid succession of further self-improvements. The term “technological singularity” is sometimes used in place of “intelligence explosion;” until January <span class='year'>2013</span>, MIRI was named “the Singularity Institute for Artificial Intelligence” and hosted an annual Singularity Summit. Since then, Yudkowsky has come to favor I. J. Good’s older term, “intelligence explosion,” to help distinguish his views from other futurist predictions, such as Ray Kurzweil’s exponential technological progress thesis.<sup><a class='footnote' id='citation2' href='MindsAnIntroduction.html#footnote2'>2</a></sup>
</p>
<p>Technologies like smarter-than-human AI seem likely to result in large societal upheavals, for the better or for the worse. Yudkowsky coined the term “Friendly AI theory” to refer to research into techniques for aligning an AGI’s preferences with the preferences of humans. At this point, very little is known about when generally intelligent software might be invented, or what safety approaches would work well in such cases. Present-day autonomous AI can already be quite challenging to verify and validate with much confidence, and many current techniques are not likely to generalize to more intelligent and adaptive systems. “Friendly AI” is therefore closer to a menagerie of basic mathematical and philosophical questions than to a well-specified set of programming objectives.
</p>
<p>As of <span class='year'>2015</span>, Yudkowsky’s views on the future of AI continue to be debated by technology forecasters and AI researchers in industry and academia, who have yet to converge on a consensus position. Nick Bostrom’s book <em>Superintelligence</em> provides a big-picture summary of the many moral and strategic questions raised by smarter-than-human AI.<sup><a class='footnote' id='citation3' href='MindsAnIntroduction.html#footnote3'>3</a></sup>
</p>
<p>For a general introduction to the field of AI, the most widely used textbook is <em>Russell and Norvig’s Artificial Intelligence: A Modern Approach</em>.<sup><a class='footnote' id='citation4' href='MindsAnIntroduction.html#footnote4'>4</a></sup> In a chapter discussing the moral and philosophical questions raised by AI, Russell and Norvig note the technical difficulty of specifying good behavior in strongly adaptive AI:
</p>
<blockquote>
<p>[Yudkowsky] asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes. We can’t just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.
</p></blockquote>
<p>Disturbed by the possibility that future progress in AI, nanotechnology, biotechnology, and other fields could endanger human civilization, Bostrom and Ćirković compiled the first academic anthology on the topic, <em>Global Catastrophic Risks</em>.<sup><a class='footnote' id='citation5' href='MindsAnIntroduction.html#footnote5'>5</a></sup> The most extreme of these are the <em>existential risks</em>, risks that could result in the permanent stagnation or extinction of humanity.<sup><a class='footnote' id='citation6' href='MindsAnIntroduction.html#footnote6'>6</a></sup>
</p>
<p>People (experts included) tend to be <em>extraordinarily bad</em> at forecasting major future events (new technologies included). Part of Yudkowsky’s goal in discussing rationality is to figure out which biases are interfering with our ability to predict and prepare for big upheavals well in advance. Yudkowsky’s contributions to the <em>Global Catastrophic Risks</em> volume, “<a class='urllink' href='https://intelligence.org/files/CognitiveBiases.pdf' rel='nofollow'>Cognitive biases potentially affecting judgement of global risks</a>” and “<a class='urllink' href='https://intelligence.org/files/AIPosNegFactor.pdf' rel='nofollow'>Artificial intelligence as a positive and negative factor in global risk</a>,” tie together his research in cognitive science and AI. Yudkowsky and Bostrom summarize near-term concerns along with long-term ones in a chapter of the <em>Cambridge Handbook of Artificial Intelligence</em>, <a class='urllink' href='http://www.nickbostrom.com/ethics/artificial-intelligence.pdf' rel='nofollow'>“The ethics of artificial intelligence.”</a><sup><a class='footnote' id='citation7' href='MindsAnIntroduction.html#footnote7'>7</a></sup>
</p>
<p>Though this is a book about <em>human</em> rationality, the topic of AI has relevance as a source of simple illustrations of aspects of human cognition. Long-term technology forecasting is also one of the more important applications of Bayesian rationality, which can model correct reasoning even in domains where the data is scarce or equivocal.
</p>
<p>Knowing the design can tell you much about the designer; and knowing the designer can tell you much about the design.
</p>
<p>We’ll begin, then, by inquiring into what our own designer can teach us about ourselves.
</p>
<p  style='text-align: center;'> <span style='font-size:144%'>❖</span>
</p>
<div class='footnotes'>
<p id='footnote1'><span class='footnote'>Gilbert Ryle, <em>The Concept of Mind</em> (University of Chicago Press, 1949).</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation1'>&#x21a9;</a></span></p><p id='footnote2'><span class='footnote'>Irving John Good, “Speculations Concerning the First Ultraintelligent Machine,” in <em>Advances in Computers</em>, ed. Franz L. Alt and Morris Rubinoff, vol. 6 (New York: Academic Press, 1965), 31–88, doi:10.1016/S0065-2458(08)60418-0.</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation2'>&#x21a9;</a></span></p><p id='footnote3'><span class='footnote'>Nick Bostrom, <em>Superintelligence: Paths, Dangers, Strategies</em> (Oxford University Press, 2014).</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation3'>&#x21a9;</a></span></p><p id='footnote4'><span class='footnote'>Stuart J. Russell and Peter Norvig, <em>Artificial Intelligence: A Modern Approach</em>, 3rd ed. (Upper Saddle River, NJ: Prentice-Hall, 2010).</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation4'>&#x21a9;</a></span></p><p id='footnote5'><span class='footnote'>Bostrom and Ćirković, <em>Global Catastrophic Risks</em>.</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation5'>&#x21a9;</a></span></p><p id='footnote6'><span class='footnote'>An example of a possible existential risk is the “grey goo” scenario, in which molecular robots designed to efficiently self-replicate do their job too well, rapidly outcompeting living organisms as they consume the Earth’s available matter.</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation6'>&#x21a9;</a></span></p><p id='footnote7'><span class='footnote'>Nick Bostrom and Eliezer Yudkowsky, “The Ethics of Artificial Intelligence,” in <em>The Cambridge Handbook of Artificial Intelligence</em>, ed. Keith Frankish and William Ramsey (New York: Cambridge University Press, 2014).</span> <span class='back_to_citation_link'><a href='MindsAnIntroduction.html#citation7'>&#x21a9;</a></span></p></div>
<div class='bottom_nav bottom_nav_sequence' > 
<p><a class='wikilink' href='Book-III-TheMachineInTheGhost.html'>Book III: The Machine in the Ghost</a>
</p>
<p><a class='wikilink' href='Contents.html'>Top</a>
</p>
<p><a class='wikilink' href='Book-III-TheMachineInTheGhost.html'>Book</a>
</p>
<p><a class='wikilink' href='The-Power-Of-Intelligence.html'><em>Interlude:</em> The Power of Intelligence</a>
</p></div>
</div>

<!--PageActionFmt--><!--/PageActionFmt-->
<!--HTMLFooter-->
</body>
</html>

